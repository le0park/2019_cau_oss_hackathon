{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackathon_team14.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc",
        "colab_type": "text"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        " *  단, 사용할 데이터셋에 따라 is_mnist만 수정\n",
        "*   모든 구현은 [2. 데이터 전처리]와 [3. 모델 생성]에서만 진행\n",
        " *  데이터 전처리 후 트레이닝, 데이터 셋은 x_train_after, x_test_after 변수명을 유지해주세요.\n",
        " *  데이터셋이 달라져도 같은 모델 구조를 유지하여야함.\n",
        "*   [4. 모델 저장]과 [5. 모델 로드 및 평가]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *  트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        " *  team_name을 제외한 다른 부분은 수정하지 말 것\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임->모든 런타임 재설정\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  모델 구조 파일 (model_structure_teamXX.json)\n",
        " *  모델 weight 파일 (model_weight_teamXX.h5)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        "* 제출 기한: **오후 6시**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2019_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드 or 알고리즘이 적발될 경우\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  두 개의 데이터셋에 대해 다른 모델 구조를 사용한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNjkTYpcl6-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import newaxis\n",
        "from skimage.transform import resize\n",
        "def resize_image(input=None, output_shape=(48, 48)):\n",
        "  resized = resize(input, output_shape, anti_aliasing=True) * 255\n",
        "  result = resized.astype(np.uint8)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1",
        "colab_type": "text"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "is_mnist = False;\n",
        "\n",
        "# 데이터셋 로드\n",
        "# x_train, y_train: 트레이닝 데이터 및 레이블\n",
        "# x_test, y_test: 테스트 데이터 및 레이블\n",
        "if is_mnist:\n",
        "  data_type = 'mnist'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # fashion MNIST 데이터셋인 경우,\n",
        "else:\n",
        "  data_type = 'cifar10'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # cifar10 데이터셋인 경우,\n",
        "\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# 인풋 데이터 타입\n",
        "input_shape = x_test.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9c2KLDBIhNQ",
        "colab_type": "text"
      },
      "source": [
        "# **2. 데이터 전처리**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJNgjaHvIhSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aecc00a7-b1e9-48d8-8586-2a772e78737d"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "channel = 1 if is_mnist else 3\n",
        "x_train_resized = np.empty(shape=(x_train.shape[0], 48, 48, channel))\n",
        "x_test_resized = np.empty(shape=(x_test.shape[0], 48, 48, channel))\n",
        "for i in np.ndindex(x_train.shape[0]):\n",
        "  if is_mnist:\n",
        "    x_train_resized[i] = resize_image(x_train[i])[:, :, newaxis]\n",
        "  else:\n",
        "    x_train_resized[i] = resize_image(x_train[i])\n",
        "print(\"x_train finished. \")\n",
        "\n",
        "for i in np.ndindex(x_test.shape[0]):\n",
        "  if is_mnist:\n",
        "    x_test_resized[i] = resize_image(x_test[i])[:, :, newaxis]\n",
        "  else:\n",
        "    x_test_resized[i] = resize_image(x_test[i])\n",
        "print(\"x_test finished. \")\n",
        "  \n",
        "  \n",
        "  \n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# 인풋 데이터 타입\n",
        "input_shape = x_test_resized.shape[1:]\n",
        "\n",
        "#   x_train = resize_image(x_train)[:, :, newaxis]\n",
        "#   y_train = resize_image(y_train)[:, :, newaxis]\n",
        "#   x_test = resize_image(x_test)[:, :, newaxis]\n",
        "#   y_test = resize_image(y_test)[:, :, newaxis]\n",
        "  \n",
        "x_train_after = x_train_resized / 255.0\n",
        "x_test_after = x_test_resized / 255.0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train finished. \n",
            "x_test finished. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9",
        "colab_type": "text"
      },
      "source": [
        "# **3. 모델 생성**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZP4eRmRqgRp",
        "colab_type": "code",
        "outputId": "3bf3d441-f40f-41a9-a6c2-f23b11bf798e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train_after.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(128, (5, 5), kernel_initializer='glorot_uniform', padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "############## Layer 1\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train_after.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(128, (5, 5), kernel_initializer='glorot_uniform',  padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "############## Layer 2\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train_after.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='glorot_uniform',  padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "############## Layer 3\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train_after.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='glorot_uniform',  padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "############## Layer 4\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train_after.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='glorot_uniform',  padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "############## Layer 5\n",
        "  \n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(184, kernel_initializer='glorot_uniform'))\n",
        "  model.add(tf.keras.layers.Activation('relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(10,  kernel_initializer='glorot_uniform'))\n",
        "  model.add(tf.keras.layers.Activation('softmax'))\n",
        "############## Layer 6 (FC Layer)\n",
        "  return model\n",
        "\n",
        "# Checkpoint callback 함수 생성\n",
        "# save_best_only: 가장 높은 검증 정확도일때만 저장\n",
        "# save_weights_only: weight만 저장\n",
        "\n",
        "model = create_model()\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "import os\n",
        "# Weight 저장 주소\n",
        "checkpoint_path = '/content/model_weight_best.h5'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                 monitor='val_acc',\n",
        "                                                 save_weight_only=False, \n",
        "                                                 save_best_only=True, \n",
        "                                                 verbose=1)\n",
        "\n",
        "\n",
        "model.fit(x_train_after, y_train, batch_size = 128, \n",
        "          epochs = 30, shuffle=True, \n",
        "          validation_data=[x_test_after, y_test],\n",
        "          callbacks=[cp_callback]\n",
        "         )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 1.8735 - acc: 0.2878\n",
            "Epoch 00001: val_acc improved from -inf to 0.45350, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 23s 464us/sample - loss: 1.8734 - acc: 0.2877 - val_loss: 1.4832 - val_acc: 0.4535\n",
            "Epoch 2/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 1.5985 - acc: 0.3828\n",
            "Epoch 00002: val_acc improved from 0.45350 to 0.53470, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 432us/sample - loss: 1.5986 - acc: 0.3828 - val_loss: 1.3027 - val_acc: 0.5347\n",
            "Epoch 3/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 1.4707 - acc: 0.4419\n",
            "Epoch 00003: val_acc improved from 0.53470 to 0.58750, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 433us/sample - loss: 1.4704 - acc: 0.4420 - val_loss: 1.1452 - val_acc: 0.5875\n",
            "Epoch 4/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 1.3103 - acc: 0.5177\n",
            "Epoch 00004: val_acc improved from 0.58750 to 0.63180, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 437us/sample - loss: 1.3098 - acc: 0.5181 - val_loss: 1.0528 - val_acc: 0.6318\n",
            "Epoch 5/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 1.1438 - acc: 0.5953\n",
            "Epoch 00005: val_acc improved from 0.63180 to 0.69150, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 436us/sample - loss: 1.1433 - acc: 0.5955 - val_loss: 0.8810 - val_acc: 0.6915\n",
            "Epoch 6/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 1.0086 - acc: 0.6545\n",
            "Epoch 00006: val_acc did not improve from 0.69150\n",
            "50000/50000 [==============================] - 22s 438us/sample - loss: 1.0090 - acc: 0.6544 - val_loss: 0.9003 - val_acc: 0.6839\n",
            "Epoch 7/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.9013 - acc: 0.6892\n",
            "Epoch 00007: val_acc improved from 0.69150 to 0.72900, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 443us/sample - loss: 0.9014 - acc: 0.6891 - val_loss: 0.7845 - val_acc: 0.7290\n",
            "Epoch 8/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.8486 - acc: 0.7113\n",
            "Epoch 00008: val_acc improved from 0.72900 to 0.73450, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 446us/sample - loss: 0.8488 - acc: 0.7113 - val_loss: 0.7878 - val_acc: 0.7345\n",
            "Epoch 9/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.7295\n",
            "Epoch 00009: val_acc improved from 0.73450 to 0.75340, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 444us/sample - loss: 0.7883 - acc: 0.7295 - val_loss: 0.7271 - val_acc: 0.7534\n",
            "Epoch 10/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.7383 - acc: 0.7505\n",
            "Epoch 00010: val_acc improved from 0.75340 to 0.76530, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 445us/sample - loss: 0.7381 - acc: 0.7507 - val_loss: 0.6906 - val_acc: 0.7653\n",
            "Epoch 11/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.7004 - acc: 0.7610\n",
            "Epoch 00011: val_acc did not improve from 0.76530\n",
            "50000/50000 [==============================] - 22s 443us/sample - loss: 0.7005 - acc: 0.7608 - val_loss: 0.7316 - val_acc: 0.7582\n",
            "Epoch 12/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.6604 - acc: 0.7768\n",
            "Epoch 00012: val_acc did not improve from 0.76530\n",
            "50000/50000 [==============================] - 22s 441us/sample - loss: 0.6605 - acc: 0.7768 - val_loss: 0.7240 - val_acc: 0.7629\n",
            "Epoch 13/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.6363 - acc: 0.7823\n",
            "Epoch 00013: val_acc improved from 0.76530 to 0.78750, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 450us/sample - loss: 0.6362 - acc: 0.7823 - val_loss: 0.6314 - val_acc: 0.7875\n",
            "Epoch 14/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.6063 - acc: 0.7934\n",
            "Epoch 00014: val_acc improved from 0.78750 to 0.78960, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 445us/sample - loss: 0.6068 - acc: 0.7931 - val_loss: 0.6314 - val_acc: 0.7896\n",
            "Epoch 15/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5913 - acc: 0.7981\n",
            "Epoch 00015: val_acc improved from 0.78960 to 0.79660, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 446us/sample - loss: 0.5912 - acc: 0.7981 - val_loss: 0.6119 - val_acc: 0.7966\n",
            "Epoch 16/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5653 - acc: 0.8085\n",
            "Epoch 00016: val_acc improved from 0.79660 to 0.80520, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 444us/sample - loss: 0.5656 - acc: 0.8084 - val_loss: 0.5890 - val_acc: 0.8052\n",
            "Epoch 17/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8126\n",
            "Epoch 00017: val_acc improved from 0.80520 to 0.80740, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 444us/sample - loss: 0.5459 - acc: 0.8126 - val_loss: 0.5726 - val_acc: 0.8074\n",
            "Epoch 18/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5325 - acc: 0.8180\n",
            "Epoch 00018: val_acc did not improve from 0.80740\n",
            "50000/50000 [==============================] - 22s 443us/sample - loss: 0.5325 - acc: 0.8180 - val_loss: 0.6459 - val_acc: 0.7940\n",
            "Epoch 19/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.8208\n",
            "Epoch 00019: val_acc improved from 0.80740 to 0.81430, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 445us/sample - loss: 0.5232 - acc: 0.8208 - val_loss: 0.5779 - val_acc: 0.8143\n",
            "Epoch 20/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.8275\n",
            "Epoch 00020: val_acc improved from 0.81430 to 0.81810, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 443us/sample - loss: 0.5049 - acc: 0.8274 - val_loss: 0.5550 - val_acc: 0.8181\n",
            "Epoch 21/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.4836 - acc: 0.8338\n",
            "Epoch 00021: val_acc did not improve from 0.81810\n",
            "50000/50000 [==============================] - 22s 442us/sample - loss: 0.4839 - acc: 0.8337 - val_loss: 0.6254 - val_acc: 0.8076\n",
            "Epoch 22/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8390\n",
            "Epoch 00022: val_acc improved from 0.81810 to 0.82120, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 443us/sample - loss: 0.4695 - acc: 0.8390 - val_loss: 0.5476 - val_acc: 0.8212\n",
            "Epoch 23/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8416\n",
            "Epoch 00023: val_acc improved from 0.82120 to 0.82450, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 442us/sample - loss: 0.4623 - acc: 0.8417 - val_loss: 0.5500 - val_acc: 0.8245\n",
            "Epoch 24/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.8432\n",
            "Epoch 00024: val_acc improved from 0.82450 to 0.83100, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 443us/sample - loss: 0.4582 - acc: 0.8431 - val_loss: 0.5301 - val_acc: 0.8310\n",
            "Epoch 25/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.4432 - acc: 0.8467\n",
            "Epoch 00025: val_acc did not improve from 0.83100\n",
            "50000/50000 [==============================] - 22s 442us/sample - loss: 0.4432 - acc: 0.8468 - val_loss: 0.5981 - val_acc: 0.8143\n",
            "Epoch 26/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8477\n",
            "Epoch 00026: val_acc improved from 0.83100 to 0.83230, saving model to /content/model_weight_best.h5\n",
            "50000/50000 [==============================] - 22s 446us/sample - loss: 0.4368 - acc: 0.8476 - val_loss: 0.5288 - val_acc: 0.8323\n",
            "Epoch 27/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4211 - acc: 0.8552\n",
            "Epoch 00027: val_acc did not improve from 0.83230\n",
            "50000/50000 [==============================] - 22s 443us/sample - loss: 0.4214 - acc: 0.8552 - val_loss: 0.5718 - val_acc: 0.8244\n",
            "Epoch 28/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.8584\n",
            "Epoch 00028: val_acc did not improve from 0.83230\n",
            "50000/50000 [==============================] - 22s 441us/sample - loss: 0.4141 - acc: 0.8584 - val_loss: 0.5529 - val_acc: 0.8272\n",
            "Epoch 29/30\n",
            "49792/50000 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8553\n",
            "Epoch 00029: val_acc did not improve from 0.83230\n",
            "50000/50000 [==============================] - 22s 439us/sample - loss: 0.4162 - acc: 0.8553 - val_loss: 0.5635 - val_acc: 0.8279\n",
            "Epoch 30/30\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.3936 - acc: 0.8637\n",
            "Epoch 00030: val_acc did not improve from 0.83230\n",
            "50000/50000 [==============================] - 22s 437us/sample - loss: 0.3936 - acc: 0.8638 - val_loss: 0.6024 - val_acc: 0.8228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f02b70726d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR",
        "colab_type": "text"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team14'\n",
        "\n",
        "# 모델의 weight 값만 저장합니다.\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "\n",
        "# 모델의 구조만을 저장합니다.\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2BoRDZ7cFl",
        "colab_type": "text"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDBwxVUx7knQ",
        "colab_type": "code",
        "outputId": "149a3d2d-0e0c-434a-b254-a09de2648221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team14'\n",
        "\n",
        "model = keras.models.load_model('/content/model_weight_best.h5')\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 234us/sample - loss: 0.5288 - acc: 0.8323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5288052093029022, 0.8323]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}